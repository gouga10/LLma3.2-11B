{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate lab\n",
    "#terminal ! huggingface-cli login --token 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\n",
    "#srun -p cscc-gpu-p -q cscc-gpu-qos --gres=gpu:1 --pty /bin/bash -i\n",
    "#jupyter lab --no-browser --port=8888 --ip=0.0.0.0\n",
    "#ssh -L 8888:gpu-01:8888 abid.abderrazek@ciai.mbzuai.ac.ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/abid.abderrazek/llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abid.abderrazek/.conda/envs/lab/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inferencing LLama 3.2 11B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abid.abderrazek/.conda/envs/lab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MllamaForConditionalGeneration(\n",
       "  (vision_model): MllamaVisionModel(\n",
       "    (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), padding=valid, bias=False)\n",
       "    (gated_positional_embedding): MllamaPrecomputedPositionEmbedding(\n",
       "      (tile_embedding): Embedding(9, 8197120)\n",
       "    )\n",
       "    (pre_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
       "      (embedding): Embedding(9, 5120)\n",
       "    )\n",
       "    (post_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
       "      (embedding): Embedding(9, 5120)\n",
       "    )\n",
       "    (layernorm_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (layernorm_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): MllamaVisionEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MllamaVisionEncoderLayer(\n",
       "          (self_attn): MllamaVisionSdpaAttention(\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaVisionMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (global_transformer): MllamaVisionEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x MllamaVisionEncoderLayer(\n",
       "          (self_attn): MllamaVisionSdpaAttention(\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaVisionMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_model): MllamaForCausalLM(\n",
       "    (model): MllamaTextModel(\n",
       "      (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (3): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (8): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (13): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (18): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (23): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (28): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (33): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (38): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (39): MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): MllamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (multi_modal_projector): Linear(in_features=7680, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "# Check if CUDA is available\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id,padding_side='left')\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id,padding_side='left')\n",
    "\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  describe the image: This image depicts the cover of a book titled \"Underground\" by David Macaulay. \n",
      "\n",
      "* The title is in large orange text at the top of the cover.\n",
      "\t+ The font is serif and all\n"
     ]
    }
   ],
   "source": [
    "# Load an image for inference (ensure the path is correct)\n",
    "\n",
    "image_path = \"as.jpg\"  # Replace with your actual image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Define your text prompt\n",
    "prompt =tokenizer.bos_token+ \"<|image|> describe the image\"\n",
    "\n",
    "# Preprocess the image and prompt\n",
    "inputs = processor(\n",
    "    images=image,\n",
    "    text=prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "# Generate outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=50)  # Adjust max_length as needed\n",
    "\n",
    "# Decode the output to text\n",
    "generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a word at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "added=''\n",
    "# Print the generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the\n"
     ]
    }
   ],
   "source": [
    "#Keep rerunning this cell\n",
    "\n",
    "prompt = prompt+added\n",
    "\n",
    "# Preprocess the image and prompt\n",
    "inputs = processor(\n",
    "    images=image,\n",
    "    text=prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(**inputs)\n",
    "\n",
    "added=tokenizer.decode(torch.argmax(outputs.logits[0,-1]))\n",
    "print(added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abid.abderrazek/.conda/envs/lab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "# Check if CUDA is available\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id,padding_side='left')\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id,padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "\n",
    "local_path =\"image.jpeg\"\n",
    "\n",
    "image = Image.open(local_path)\n",
    "\n",
    "prompt=[\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "       {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"what is the person wearing.\"}\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"the person is\"}\n",
    "    ]}\n",
    "]\n",
    "answer=' wearing a black t shirt with an orange'\n",
    "prompt_processed = processor.apply_chat_template(prompt,continue_final_message=True,tokenize=False)\n",
    "full_response =prompt_processed+answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the batches (suitable for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_answer_tokens=len(tokenizer.tokenize(answer))+1\n",
    "images=[image for _ in range(1,nbr_answer_tokens)]\n",
    "\n",
    "inputs=[]\n",
    "outputs=[full_response]\n",
    "input=full_response\n",
    "for i in range(1,nbr_answer_tokens):\n",
    "        \n",
    "    input=tokenizer.decode(tokenizer.encode(input)[: -1])\n",
    "    inputs.append(input)\n",
    "\n",
    "\n",
    "full_response_ids = processor(images,inputs, return_tensors=\"pt\",add_special_tokens=False,padding='max_length',max_length=len(tokenizer.tokenize(full_response)),padding_side='left').to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(**full_response_ids,max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black t shirt with an\n",
      "-----------\n",
      "2 generated tokens :   orange logo\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black t shirt with\n",
      "-----------\n",
      "2 generated tokens :   an orange\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black t shirt\n",
      "-----------\n",
      "2 generated tokens :   with an\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black t\n",
      "-----------\n",
      "2 generated tokens :  -shirt with\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black\n",
      "-----------\n",
      "2 generated tokens :   t-shirt\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a\n",
      "-----------\n",
      "2 generated tokens :   pair of\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing\n",
      "-----------\n",
      "2 generated tokens :   a pair\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is\n",
      "-----------\n",
      "2 generated tokens :   wearing a\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for output,input in zip(outputs,full_response_ids['input_ids']):\n",
    "    print('input text  :  ')\n",
    "    print(tokenizer.decode(input))\n",
    "    print('-----------')\n",
    "    print('2 generated tokens : ',tokenizer.decode(output[-2 :]))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('----------------------------------- new example ----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!! orange\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!! an orange\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!! with an orange\n",
      "!!!!!!!!!!!!!!!!!!!!!!!! shirt with an orange\n",
      "!!!!!!!!!!!!!!!!!!!!!!! t shirt with an orange\n",
      "!!!!!!!!!!!!!!!!!!!!!! black t shirt with an orange\n",
      "!!!!!!!!!!!!!!!!!!!!! a black t shirt with an orange\n",
      "!!!!!!!!!!!!!!!!!!!! wearing a black t shirt with an orange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating the labels \n",
    "\n",
    "label=tokenizer(answer,add_special_tokens=False,return_tensors=\"pt\",padding_side='left',padding='max_length',max_length=full_response_ids['input_ids'].shape[1])['input_ids']\n",
    "\n",
    "\n",
    "for i in range(1,nbr_answer_tokens):\n",
    "    t=label.clone()\n",
    "    t[0,:-i]=0\n",
    "    if i ==1:\n",
    "        labels=t.clone()\n",
    "    else:\n",
    "        \n",
    "        labels=torch.cat((labels,t),dim=0)\n",
    "\n",
    "for label in labels:\n",
    "    print(tokenizer.decode(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the -100 tokens will be ignored when calculating the loss \n",
    "\n",
    "labels_tokenized_processed=torch.where(labels != 0,labels,-100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**full_response_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def calculate_loss(logits,labels):\n",
    "    loss_fn=nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss=loss_fn(logits.view(-1,logits.size(-1)),labels.view(-1))\n",
    "    return cross_entropy_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=calculate_loss(outputs.logits.to(device),labels_tokenized_processed.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3946e-02],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.2965e+00, 3.5347e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 9.4250e+00, 1.4516e+01, 6.2425e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.1978e+01, 1.1389e+01, 1.0627e+01, 1.5908e+01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2160e+01,\n",
       "         1.1282e+01, 1.0736e+01, 1.1270e+01, 1.0488e+01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2699e+01, 8.1064e+00,\n",
       "         1.3734e+01, 1.0865e+01, 4.9523e+00, 1.0215e+01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5987e+01, 1.4336e+01, 1.3329e+01,\n",
       "         9.4268e+00, 7.0949e+00, 7.5728e+00, 5.9767e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.2412e+01, 1.0020e+01, 1.5465e+01, 1.3611e+01,\n",
       "         1.2531e+01, 1.1653e+01, 1.1692e+01, 1.2319e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.view(full_response_ids['input_ids'].shape[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer=AdamW(model.parameters(),lr=1e-4,weight_decay=0.01)\n",
    "\n",
    "for _ in range(10):\n",
    "    out=model(**full_response_ids)\n",
    "    loss=calculate_loss(out.logits,labels_tokenized_processed.to(device)).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print('loss : ',loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    "\n",
    ").to(device)\n",
    "\n",
    "from peft import LoraConfig,get_peft_model\n",
    "\n",
    "lora_config=LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj','v_proj']\n",
    "\n",
    ")\n",
    "model=get_peft_model(model,lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer=AdamW(model.parameters(),lr=1e-4,weight_decay=0.01)\n",
    "\n",
    "for _ in range(10):\n",
    "    out=model(**full_response_ids)\n",
    "    loss=calculate_loss(out.logits,labels_tokenized_processed.to(device)).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print('loss : ',loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide image paths\n",
    "local_paths =[\"image.jpeg\",'as.jpg']\n",
    "\n",
    "\n",
    "#provide prompts in this format\n",
    "prompts=[\n",
    "    [{\"role\": \"user\", \"content\": [\n",
    "       {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"what is the person wearing.\"}\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"the person is\"}\n",
    "    ]}],\n",
    "    [{\"role\": \"user\", \"content\": [\n",
    "       {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"what is the image\"}\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"it is\"}\n",
    "    ]}]\n",
    "]\n",
    "#provide the wanted answers\n",
    "answers=[' wearing a black t-shirt.',' a book cover about underground.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create dataloaders of size batchsize\n",
    "\n",
    "\n",
    "def preprocess(prompts,answers,image_paths,batch_size):\n",
    "\n",
    "\n",
    "    independant_images=[Image.open(path) for path in image_paths]\n",
    "    all_images,inputs,processed_inputs,unprocessed_labels,words_to_predict,processed_labels=[],[],[],[],[],[]\n",
    "    \n",
    "    word_to_predict=None\n",
    "\n",
    "    prompts=processor.apply_chat_template(prompts,continue_final_message=True,tokenize=False)\n",
    "    independant_sequences=[prompt+answer for prompt,answer in zip(prompts,answers)]\n",
    "    for image , independant_sequence,answer in zip(independant_images,independant_sequences,answers):\n",
    "        nbr_answer_tokens=len(tokenizer.tokenize(answer))+1\n",
    "        same_seq_images=[image for _ in range(1,nbr_answer_tokens)]\n",
    "        all_images.extend(same_seq_images)\n",
    "        \n",
    "        input=independant_sequence\n",
    "        for _ in range(1,nbr_answer_tokens):\n",
    "            word_to_predict=tokenizer.decode(tokenizer.encode(input)[-1],padding_side='left') \n",
    "            input=tokenizer.decode(tokenizer.encode(input)[: -1],padding_side='left')\n",
    "            inputs.append(input)\n",
    "\n",
    "            words_to_predict.append(tokenizer(word_to_predict,add_special_tokens=False,return_tensors=\"pt\",padding_side='left',padding='max_length',max_length=max([len(tokenizer.encode(i)) for i in independant_sequences]))['input_ids'])\n",
    "        \n",
    "    for tokenized_label in words_to_predict:\n",
    "        \n",
    "        unprocessed_labels.append(torch.where(tokenized_label != 128004,tokenized_label,-100))\n",
    "    unprocessed_labels = torch.cat(unprocessed_labels, dim=0)\n",
    "    \n",
    "    for i in range (0,len(inputs),batch_size):\n",
    "        processed_inputs.append(processor(all_images[i:i+batch_size],inputs[i:i+batch_size], return_tensors=\"pt\",add_special_tokens=False,padding='max_length',max_length=max([len(tokenizer.encode(i)) for i in independant_sequences]),padding_side='left'))\n",
    "        processed_labels.append(unprocessed_labels[i:i+batch_size])\n",
    "        #processed_labels = torch.cat(processed_labels, dim=0)\n",
    "\n",
    "    return(processed_inputs,processed_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,labels=preprocess(prompts=prompts,answers=answers,image_paths=local_paths,batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text  :  \n",
      "<|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "it is a book cover\n",
      "-----------\n",
      "2 generated tokens :   for the\n",
      "---------------------------------------------------------------------------\n",
      "target label :   about\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "it is a book\n",
      "-----------\n",
      "2 generated tokens :   by David\n",
      "---------------------------------------------------------------------------\n",
      "target label :   cover\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "it is a\n",
      "-----------\n",
      "2 generated tokens :   book cover\n",
      "---------------------------------------------------------------------------\n",
      "target label :   book\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "it is\n",
      "-----------\n",
      "2 generated tokens :   a book\n",
      "---------------------------------------------------------------------------\n",
      "target label :   a\n",
      "----------------------------------- new example ----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Generate and assess responses compared to wanted answers\n",
    "\n",
    "batch=2   # which batch to test\n",
    "\n",
    "nbr_of_tokens=2  #nbr of tokens to generate and assess\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs[batch].to(device),max_new_tokens=nbr_of_tokens)\n",
    "for output,input,label in zip(outputs,inputs[batch]['input_ids'],labels[batch]):\n",
    "    print('input text  :  ')\n",
    "    print(tokenizer.decode(input))\n",
    "    print('-----------')\n",
    "    print(f'{nbr_of_tokens} generated tokens : ',tokenizer.decode(output[-nbr_of_tokens:]))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('target label : ',tokenizer.decode(label[-1]))\n",
    "    print('----------------------------------- new example ----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 6.4591],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3568],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3903],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.9539]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch=2\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(**inputs[batch].to(device))\n",
    "loss=calculate_loss(outputs.logits.to(device),labels[batch].to(device))\n",
    "loss.view(inputs[0]['input_ids'].shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text  :  \n",
      "<|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "it is a book cover\n",
      "-----------\n",
      "1 generated token :   for\n",
      "---------------------------------------------------------------------------\n",
      "target label :   about\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "it is a book\n",
      "-----------\n",
      "1 generated token :   by\n",
      "---------------------------------------------------------------------------\n",
      "target label :   cover\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "it is a\n",
      "-----------\n",
      "1 generated token :   book\n",
      "---------------------------------------------------------------------------\n",
      "target label :   book\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "it is\n",
      "-----------\n",
      "1 generated token :   a\n",
      "---------------------------------------------------------------------------\n",
      "target label :   a\n",
      "----------------------------------- new example ----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for output,input,label in zip(outputs.logits,inputs[batch]['input_ids'],labels[batch]):\n",
    "    print('input text  :  ')\n",
    "    print(tokenizer.decode(input))\n",
    "    print('-----------')\n",
    "    print('1 generated token : ',tokenizer.decode(torch.argmax(output[-1])))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('target label : ',tokenizer.decode(label[-1]))\n",
    "    print('----------------------------------- new example ----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
