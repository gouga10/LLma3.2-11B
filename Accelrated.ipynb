{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `kaggle` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `kaggle`\n"
     ]
    }
   ],
   "source": [
    "#conda activate lab\n",
    "#! huggingface-cli login --token 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\n",
    "#srun -p cscc-gpu-p -q cscc-gpu-qos --gres=gpu:1 --pty /bin/bash -i\n",
    "#jupyter lab --no-browser --port=8888 --ip=0.0.0.0\n",
    "#ssh -L 8888:gpu-01:8888 abid.abderrazek@ciai.mbzuai.ac.ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft) (2.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, accelerate, transformers, peft\n",
      "Successfully installed accelerate-1.1.1 huggingface-hub-0.26.2 peft-0.13.2 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers peft accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inferencing LLama 3.2 11B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "# Check if CUDA is available\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id,padding_side='left')\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id,padding_side='left')\n",
    "\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  describe the image: This image depicts the cover of a book titled \"Underground\" by David Macaulay. \n",
      "\n",
      "* The title is in large orange text at the top of the cover.\n",
      "\t+ The font is serif and all\n"
     ]
    }
   ],
   "source": [
    "# Load an image for inference (ensure the path is correct)\n",
    "\n",
    "image_path = \"as.jpg\"  # Replace with your actual image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Define your text prompt\n",
    "prompt =tokenizer.bos_token+ \"<|image|> describe the image\"\n",
    "\n",
    "# Preprocess the image and prompt\n",
    "inputs = processor(\n",
    "    images=image,\n",
    "    text=prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "# Generate outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=50)  # Adjust max_length as needed\n",
    "\n",
    "# Decode the output to text\n",
    "generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a word at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "added=''\n",
    "# Print the generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the\n"
     ]
    }
   ],
   "source": [
    "#Keep rerunning this cell\n",
    "\n",
    "prompt = prompt+added\n",
    "\n",
    "# Preprocess the image and prompt\n",
    "inputs = processor(\n",
    "    images=image,\n",
    "    text=prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(**inputs)\n",
    "\n",
    "added=tokenizer.decode(torch.argmax(outputs.logits[0,-1]))\n",
    "print(added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abid.abderrazek/.conda/envs/lab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "# Check if CUDA is available\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id,padding_side='left')\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id,padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "\n",
    "local_path =\"image.jpeg\"\n",
    "\n",
    "image = Image.open(local_path)\n",
    "\n",
    "prompt=[\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "       {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"what is the person wearing.\"}\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"the person is\"}\n",
    "    ]}\n",
    "]\n",
    "answer=' wearing a black t shirt with an orange'\n",
    "prompt_processed = processor.apply_chat_template(prompt,continue_final_message=True,tokenize=False)\n",
    "full_response =prompt_processed+answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the batches (suitable for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_answer_tokens=len(tokenizer.tokenize(answer))+1\n",
    "images=[image for _ in range(1,nbr_answer_tokens)]\n",
    "\n",
    "inputs=[]\n",
    "outputs=[full_response]\n",
    "input=full_response\n",
    "for i in range(1,nbr_answer_tokens):\n",
    "        \n",
    "    input=tokenizer.decode(tokenizer.encode(input)[: -1])\n",
    "    inputs.append(input)\n",
    "\n",
    "\n",
    "full_response_ids = processor(images,inputs, return_tensors=\"pt\",add_special_tokens=False,padding='max_length',max_length=len(tokenizer.tokenize(full_response)),padding_side='left').to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(**full_response_ids,max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black t shirt with an\n",
      "-----------\n",
      "2 generated tokens :   orange logo\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black t shirt with\n",
      "-----------\n",
      "2 generated tokens :   an orange\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black t shirt\n",
      "-----------\n",
      "2 generated tokens :   with an\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black t\n",
      "-----------\n",
      "2 generated tokens :  -shirt with\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a black\n",
      "-----------\n",
      "2 generated tokens :   t-shirt\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing a\n",
      "-----------\n",
      "2 generated tokens :   pair of\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is wearing\n",
      "-----------\n",
      "2 generated tokens :   a pair\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n",
      "input text  :  \n",
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>what is the person wearing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "the person is\n",
      "-----------\n",
      "2 generated tokens :   wearing a\n",
      "---------------------------------------------------------------------------\n",
      "----------------------------------- new example ----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for output,input in zip(outputs,full_response_ids['input_ids']):\n",
    "    print('input text  :  ')\n",
    "    print(tokenizer.decode(input))\n",
    "    print('-----------')\n",
    "    print('2 generated tokens : ',tokenizer.decode(output[-2 :]))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('----------------------------------- new example ----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!! orange\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!! an orange\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!! with an orange\n",
      "!!!!!!!!!!!!!!!!!!!!!!!! shirt with an orange\n",
      "!!!!!!!!!!!!!!!!!!!!!!! t shirt with an orange\n",
      "!!!!!!!!!!!!!!!!!!!!!! black t shirt with an orange\n",
      "!!!!!!!!!!!!!!!!!!!!! a black t shirt with an orange\n",
      "!!!!!!!!!!!!!!!!!!!! wearing a black t shirt with an orange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating the labels \n",
    "\n",
    "label=tokenizer(answer,add_special_tokens=False,return_tensors=\"pt\",padding_side='left',padding='max_length',max_length=full_response_ids['input_ids'].shape[1])['input_ids']\n",
    "\n",
    "\n",
    "for i in range(1,nbr_answer_tokens):\n",
    "    t=label.clone()\n",
    "    t[0,:-i]=0\n",
    "    if i ==1:\n",
    "        labels=t.clone()\n",
    "    else:\n",
    "        \n",
    "        labels=torch.cat((labels,t),dim=0)\n",
    "\n",
    "for label in labels:\n",
    "    print(tokenizer.decode(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the -100 tokens will be ignored when calculating the loss \n",
    "\n",
    "labels_tokenized_processed=torch.where(labels != 0,labels,-100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**full_response_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def calculate_loss(logits,labels):\n",
    "    loss_fn=nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss=loss_fn(logits.view(-1,logits.size(-1)),labels.view(-1))\n",
    "    return cross_entropy_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=calculate_loss(outputs.logits.to(device),labels_tokenized_processed.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3946e-02],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.2965e+00, 3.5347e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 9.4250e+00, 1.4516e+01, 6.2425e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.1978e+01, 1.1389e+01, 1.0627e+01, 1.5908e+01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2160e+01,\n",
       "         1.1282e+01, 1.0736e+01, 1.1270e+01, 1.0488e+01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2699e+01, 8.1064e+00,\n",
       "         1.3734e+01, 1.0865e+01, 4.9523e+00, 1.0215e+01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5987e+01, 1.4336e+01, 1.3329e+01,\n",
       "         9.4268e+00, 7.0949e+00, 7.5728e+00, 5.9767e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.2412e+01, 1.0020e+01, 1.5465e+01, 1.3611e+01,\n",
       "         1.2531e+01, 1.1653e+01, 1.1692e+01, 1.2319e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.view(full_response_ids['input_ids'].shape[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer=AdamW(model.parameters(),lr=1e-4,weight_decay=0.01)\n",
    "\n",
    "for _ in range(10):\n",
    "    out=model(**full_response_ids)\n",
    "    loss=calculate_loss(out.logits,labels_tokenized_processed.to(device)).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print('loss : ',loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    "\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id,padding_side='left')\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id,padding_side='left')\n",
    "from peft import LoraConfig,get_peft_model\n",
    "\n",
    "lora_config=LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj','v_proj']\n",
    "\n",
    ")\n",
    "model=get_peft_model(model,lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer=AdamW(model.parameters(),lr=1e-4,weight_decay=0.01)\n",
    "\n",
    "for _ in range(10):\n",
    "    out=model(**full_response_ids)\n",
    "    loss=calculate_loss(out.logits,labels_tokenized_processed.to(device)).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print('loss : ',loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom batched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abid.abderrazek/.conda/envs/lab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 47,185,920 || all params: 10,717,406,755 || trainable%: 0.4403\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    "\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id,padding_side='left')\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id,padding_side='left')\n",
    "from peft import LoraConfig,get_peft_model\n",
    "\n",
    "lora_config=LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj','v_proj']\n",
    "\n",
    ")\n",
    "model=get_peft_model(model,lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('image_paths.json','r')as f:\n",
    "    image_paths=json.load(f)\n",
    "with open('answers.json','r')as f:\n",
    "    answers=json.load(f)\n",
    "with open('templates.json','r')as f:\n",
    "    prompts=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/S005C001P010R001A043_rgb_0.jpeg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths=['images/'+img for img in image_paths]\n",
    "image_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataloader for single GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(prompts,answers,image_paths,batch_size):\n",
    "\n",
    "\n",
    "    independant_images=[Image.open(path) for path in image_paths]\n",
    "    all_images,inputs,processed_inputs,unprocessed_labels,words_to_predict,processed_labels=[],[],[],[],[],[]\n",
    "    \n",
    "    word_to_predict=None\n",
    "\n",
    "    prompts=processor.apply_chat_template(prompts,continue_final_message=True,tokenize=False)\n",
    "    independant_sequences=[prompt+answer for prompt,answer in zip(prompts,answers)]\n",
    "    for image , independant_sequence,answer in zip(independant_images,independant_sequences,answers):\n",
    "        nbr_answer_tokens=len(tokenizer.tokenize(answer))+1\n",
    "        same_seq_images=[image for _ in range(1,nbr_answer_tokens)]\n",
    "        all_images.extend(same_seq_images)\n",
    "        \n",
    "        input=independant_sequence\n",
    "        \n",
    "        for _ in range(1,nbr_answer_tokens):\n",
    "            word_to_predict=tokenizer.decode(tokenizer.encode(input)[-1],padding_side='left') \n",
    "            input=tokenizer.decode(tokenizer.encode(input)[: -1],padding_side='left')\n",
    "            inputs.append(input)\n",
    "\n",
    "            words_to_predict.append(tokenizer(word_to_predict,add_special_tokens=False,return_tensors=\"pt\",padding_side='left',padding='max_length',max_length=max([len(tokenizer.encode(i)) for i in independant_sequences]))['input_ids'])\n",
    "        \n",
    "    for tokenized_label in words_to_predict:\n",
    "        \n",
    "        unprocessed_labels.append(torch.where(tokenized_label != 128004,tokenized_label,-100))\n",
    "    unprocessed_labels = torch.cat(unprocessed_labels, dim=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range (0,len(inputs),batch_size):\n",
    "        processed_inputs.append(processor(all_images[i:i+batch_size],inputs[i:i+batch_size], return_tensors=\"pt\",add_special_tokens=False,padding='max_length',max_length=max([len(tokenizer.encode(i)) for i in independant_sequences]),padding_side='left'))\n",
    "        processed_labels.append(unprocessed_labels[i:i+batch_size])\n",
    "        #processed_labels = torch.cat(processed_labels, dim=0)\n",
    "\n",
    "    \n",
    "    return(processed_inputs,processed_labels)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "inputs,labels=preprocess(prompts=prompts[:10],answers=answers[:10],image_paths=image_paths[:10],batch_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate and assess responses compared to wanted answers\n",
    "\n",
    "batch=2   # which batch to test\n",
    "\n",
    "nbr_of_tokens=2  #nbr of tokens to generate and assess\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs[batch].to(device),max_new_tokens=nbr_of_tokens)\n",
    "for output,input,label in zip(outputs,inputs[batch]['input_ids'],labels[batch]):\n",
    "    print('input text  :  ')\n",
    "    print(tokenizer.decode(input))\n",
    "    print('-----------')\n",
    "    print(f'{nbr_of_tokens} generated tokens : ',tokenizer.decode(output[-nbr_of_tokens:]))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('target label : ',tokenizer.decode(label[-1]))\n",
    "    print('----------------------------------- new example ----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def calculate_loss(logits,labels):\n",
    "    loss_fn=nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss=loss_fn(logits.view(-1,logits.size(-1)),labels.view(-1))\n",
    "    return cross_entropy_loss\n",
    "\n",
    "\n",
    "\n",
    "batch=3\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(**inputs[batch].to(device))\n",
    "loss=calculate_loss(outputs.logits.to(device),labels[batch].to(device))\n",
    "loss.view(inputs[0]['input_ids'].shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output,input,label in zip(outputs.logits,inputs[batch]['input_ids'],labels[batch]):\n",
    "    print('input text  :  ')\n",
    "    print(tokenizer.decode(input))\n",
    "    print('-----------')\n",
    "    print('1 generated token : ',tokenizer.decode(torch.argmax(output[-1])))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('target label : ',tokenizer.decode(label[-1]))\n",
    "    print('----------------------------------- new example ----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataloader for DDP   (Better to use FineTuning_DDP.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abid.abderrazek/.conda/envs/lab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 47,185,920 || all params: 10,717,406,755 || trainable%: 0.4403\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id , torch_dtype=torch.bfloat16)\n",
    "processor = AutoProcessor.from_pretrained(model_id,padding_side='left')\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id,padding_side='left')\n",
    "from peft import LoraConfig,get_peft_model\n",
    "\n",
    "lora_config=LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    r=64,\n",
    "    use_rslora=True,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj','v_proj']\n",
    "\n",
    ")\n",
    "model=get_peft_model(model,lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
    "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    \n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    print('local ddp',ddp_local_rank)\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla, non-DDP run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    # attempt to autodetect device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"using device: {device}\")\n",
    "\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "\n",
    "import json\n",
    "with open('llama_stuff/image_paths.json','r')as f:\n",
    "    image_paths=json.load(f)\n",
    "with open('llama_stuff/answers.json','r')as f:\n",
    "    answers=json.load(f)\n",
    "with open('llama_stuff/templates.json','r')as f:\n",
    "    prompts=json.load(f)\n",
    "\n",
    "image_paths=['./llama_stuff/2nd_exp/2nd_FT_images/'+img for img in image_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "max_length=0\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, prompts, answers, image_paths, processor, tokenizer):\n",
    "        self.prompts = processor.apply_chat_template(prompts, continue_final_message=True, tokenize=False)\n",
    "        self.answers = answers\n",
    "        self.image_paths = image_paths\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.independant_sequences = [prompt + answer +tokenizer.eos_token for prompt, answer in zip(self.prompts, self.answers)]\n",
    "        self.independant_images = [Image.open(path) for path in image_paths]\n",
    "        self.all_data = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \n",
    "        all_images,inputs,processed_inputs,unprocessed_labels,words_to_predict,processed_labels=[],[],[],[],[],[]\n",
    "        all_data=[]\n",
    "        word_to_predict=None\n",
    "\n",
    "        for image , independant_sequence,answer in zip(self.independant_images,self.independant_sequences,self.answers):\n",
    "            nbr_answer_tokens=len(tokenizer.tokenize(answer))\n",
    "            \n",
    "            \n",
    "            same_seq_images=[image for _ in range(0,nbr_answer_tokens)]\n",
    "            all_images.extend(same_seq_images)\n",
    "            input=independant_sequence\n",
    "            for _ in range(0,nbr_answer_tokens):\n",
    "                word_to_predict=tokenizer.decode(tokenizer.encode(input)[-1],padding_side='left') \n",
    "                input=tokenizer.decode(tokenizer.encode(input)[: -1],padding_side='left')\n",
    "                inputs.append(input)\n",
    "\n",
    "                tokenized_label=tokenizer(word_to_predict,add_special_tokens=False,return_tensors=\"pt\",padding_side='left',padding='max_length',max_length=max([len(tokenizer.encode(i)) for i in self.independant_sequences]))['input_ids']\n",
    "                unprocessed_labels.append(torch.where(tokenized_label != 128004,tokenized_label,-100))\n",
    "            \n",
    "            \n",
    "            processed_labels = torch.cat(unprocessed_labels, dim=0)\n",
    "            #print(processed_labels)\n",
    "\n",
    "\n",
    "        for image,input_seq,label in zip(all_images,inputs,processed_labels):\n",
    "            all_data.append({\n",
    "                        'image': image,\n",
    "                        'input_seq': input_seq,\n",
    "                        'label': label  # Remove extra dimension\n",
    "                    })\n",
    "        return all_data\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.all_data[idx]\n",
    "        return item['image'], item['input_seq'], item['label']\n",
    "    \n",
    "\n",
    "# Collate function for batch processing\n",
    "def collate_fn(batch):\n",
    "    images, inputs, labels = zip(*batch)\n",
    "    # Process images and inputs using the processor\n",
    "    processed_inputs = processor(\n",
    "        images, inputs, \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=False, \n",
    "        padding='max_length', \n",
    "        max_length=max(label.size(0) for label in labels), \n",
    "        padding_side='left'\n",
    "    )\n",
    "     # Stack labels into a tensor\n",
    "    stacked_labels = torch.stack(labels)\n",
    "    return processed_inputs, stacked_labels\n",
    "\n",
    "# Dataset and DataLoader setup\n",
    "dataset = CustomDataset(prompts[:10], answers[:10], image_paths[:10], processor, tokenizer)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch_idx, (processed_inputs, batch_labels) in enumerate(data_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    input_ids = processed_inputs['input_ids']\n",
    "    attention_mask = processed_inputs['attention_mask']  # Assuming it's part of the processed data\n",
    "    \n",
    "    for idx in range(input_ids.size(0)):  # Iterate over each sequence in the batch\n",
    "        print(f\"  Sequence {idx + 1}:\")\n",
    "        print(f\"    Input: {tokenizer.decode(input_ids[idx])}\")\n",
    "        print('---------')\n",
    "        print(f\"    Label: {tokenizer.decode(batch_labels[idx][-1])}\")\n",
    "        print('---------')\n",
    "\n",
    "    # Break after the first batch for demonstration (optional)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "optimizer=AdamW(model.parameters(),lr=1e-4,weight_decay=0.01)\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(data_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "      \"linear\",\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=num_training_steps\n",
    "  )\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return cross_entropy_loss\n",
    "model.train()\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_accum = torch.zeros(1,1).to(device)\n",
    "    for batch_idx, (processed_inputs, batch_labels) in enumerate(data_loader):\n",
    "        if   (ddp_local_rank*len(data_loader))//ddp_world_size  <= batch_idx < (ddp_local_rank+1)*len(data_loader)//ddp_world_size:\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            marco=processed_inputs.to(device)\n",
    "            # added after video, this field is also used by the forward pass.\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = True\n",
    "            outputs = model(**marco)\n",
    "            loss=calculate_loss(outputs.logits,batch_labels.to(device)).mean()\n",
    "            loss.backward()\n",
    "            loss_accum += loss.detach().to(device)\n",
    "            \n",
    "            if ddp:\n",
    "                dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "                norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                if device_type == \"cuda\":\n",
    "                    torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            del marco\n",
    "            del batch_labels \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            # if master_process:\n",
    "            #     print(f\"device : {device}   | loss: {loss.item():.6f}  | epoch {epoch}/{num_epochs} |  batch : {batch_idx}\")\n",
    "        \n",
    "            \n",
    "if master_process:\n",
    "    save_path = \"lora_checkpoint\"\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        raw_model = model.module\n",
    "    else:\n",
    "        raw_model = model\n",
    "\n",
    "    # Save only the LoRA parameters\n",
    "    print(f'gpu {ddp_local_rank} saved lora checkpoint')\n",
    "    raw_model.save_pretrained(save_path)\n",
    "\n",
    "if device_type == \"cuda\":\n",
    "        print(f'gpu {ddp_local_rank} waiting')\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()\n",
    "    torch.cuda.empty_cache() \n",
    "    print(f'gpu {ddp_local_rank} exited')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
